# 📌 1. install Lab
!pip install xgboost openpyxl optuna scikit-learn matplotlib seaborn
#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# 📌 2. import
import pandas as pd
import numpy as np
import xgboost as xgb
import optuna
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# 📌 3. Upload training file
print("⬆️ Please upload the training file (train.xlsx):")
uploaded = files.upload()
file_path = next(iter(uploaded))
train_df = pd.read_excel(file_path)

# Remove non-numeric columns like time
train_df = train_df.select_dtypes(include=[np.number])

# Separation of features and purpose
X = train_df.drop(columns=["Result"], errors='ignore')
y = train_df["Result"]

# Class mapping for XGBoost (to 0, 1, and 2)
label_map = {-1: 0, 0: 1, 1: 2}
y = y.map(label_map)

# Data sharing
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)
#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# 📌 4. Preprocessing: Removing extra columns and preparing data
#        Remove columns that the model cannot work with.
if 'Time' in train_df.columns:
    train_df = train_df.drop(columns=['Time'])

# Separating purpose and features
y = train_df["Result"]
X = train_df.drop(columns=["Result", "Profit"], errors='ignore')

# Mapping classes to understandable numbers for XGBoost
label_map = {-1: 0, 0: 1, 1: 2}
y_mapped = y.map(label_map)

# Splitting data for training and validation
X_train, X_valid, y_train, y_valid = train_test_split(X, y_mapped, test_size=0.2, random_state=42)
#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# 📌 5. Optuna function definition for XGBoost
def objective(trial):
    params = {
        "objective": "multi:softprob",
        "num_class": 3,
        "eval_metric": "mlogloss",
        "booster": "gbtree",
        "tree_method": "auto",
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
        "gamma": trial.suggest_float("gamma", 0, 5),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
        "verbosity": 0
    }

    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    model = xgb.train(
        params,
        dtrain,
        num_boost_round=1000,
        evals=[(dvalid, "valid")],
        early_stopping_rounds=30,
        verbose_eval=False
    )

    preds = model.predict(dvalid)
    pred_labels = np.argmax(preds, axis=1)
    acc = accuracy_score(y_valid, pred_labels)
    return acc
#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# 📌 6. Run Optuna
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)
#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# 📌 7. Training the final model with the best parameters
best_params = study.best_params
best_params.update({
    "objective": "multi:softprob",
    "num_class": 3,
    "eval_metric": "mlogloss",
    "verbosity": 0
})

dtrain_final = xgb.DMatrix(X, label=y_mapped)
final_model = xgb.train(best_params, dtrain_final, num_boost_round=study.best_trial.number)
#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# 📌 8. Upload test file via user
uploaded_test = files.upload()

df_test = pd.read_excel(next(iter(uploaded_test)))
if 'Time' in df_test.columns:
    df_test = df_test.drop(columns=['Time'])

X_test = df_test.copy()
#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# 📌 9. Prediction on test data
dtest = xgb.DMatrix(X_test)
pred_probs = final_model.predict(dtest)
pred_labels = np.argmax(pred_probs, axis=1)
#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# 📌 10. Restoring classes to their original state
reverse_map = {0: -1, 1: 0, 2: 1}
pred_result = pd.Series(pred_labels).map(reverse_map)
#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# 📤 11. Prediction on test and output
df_test["Result"] = pred_result
df_test.to_excel("predicted_test.xlsx", index=False)
#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# 📦 12.Download the output file
files.download("predicted_test.xlsx")
